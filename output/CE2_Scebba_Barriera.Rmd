---
title: "CE2_Scebba_Barriera"
author: "Barriera G. & Scebba S."
date: "2022-12-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 
Spider-Wo-Men: with great power comes greatresponsibility

*Task 1*

A robots.txt file can be visualised for any given website just by typing the full URL for the homepage and then adding /robots.txt.

While we were trying to crawl, after typing <https://beppegrillo.it/robots.txt> we noticed that the result was not found, in fact the page that we visualized was blank with a single title: "404 not found", which means that it might be on located at a different URL or just it's not found in the website.

*Task 2*

Using the function:

```{r, eval=FALSE}
beppe_page <- RCurl::getURL(url=url, httpheader=c(from=email, 'User-Agent'= user_agent))
```

we downloaded the page by informing the webmaster about our browser details, by submitting our: url, email and user-agent.

*Task 3*

After downloading the page using these functions:

```{r, eval=FALSE}
url_beppe_blog <- "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/"

download.file(url, destfile = here::here("beppeblog.html"))

links <- XML::getHTMLLinks(doc= beppe_page, externalOnly = T, relative = F)
```

Then we created the data frame by using the function:

```{r, eval=FALSE}
links2 <- tibble(links)
```

After we got all the links (104), we wanted to keep only those that re-direct to other posts of the blog itself, in order to do so we used a regular expression by the package "stringr":

```{r, eval=FALSE}
beppe_posts <- str_extract_all(links, pattern= "^https://beppegrillo.it/.")

```

*Task 4*

We look at the beppegrillo.it archive of all the posts of 2016. 

For each of the 47 pages, we got the link and placed it into a character vector:

```{r, eval=FALSE}
links_2016 <- str_c("https://beppegrillo.it/category/archivio/2016/page/", 1:47) 

```

For each single linked blog post we wanted to download the page as a file. In order to do so, we created a "download_politely" function:

```{r, eval=FALSE}
download_politely <- function(from_url, to_html , my_email, my_agent = R.Version()$version.string) {
  
require(httr)
  
  stopifnot(is.character(from_url))
  stopifnot(is.character(to_html))
  stopifnot(is.character(my_email))

req <- httr::GET(url = from_url,
                  add_headers(
                    From = my_email,
                    `User-Agent` = R.Version()$version.string
                  ))


if (httr::http_status(req)$message == "Success: (200) OK") {
  bin <- content(req, as = "raw")
  writeBin(object = bin, con = to_html)
  } else {
  cat("Houston, we have a problem!")
  }
}

```

Then we created a loop to download each page

```{r, eval=FALSE}
for (i in seq_along(links_2016)) {
  cat(i, " ")
  
  download_politely(from_url = links_2016[i],
                    to_html = here::here("post_2016", str_c("page_",i,".html")),
                    my_email= "barrieragaia.uni@gmail.com")
  
  Sys.sleep(2)
}

```

At last, we scraped the main text for each downloaded page 

```{r, eval=FALSE}
scrape <- list.files(here::here("post_2016"), full.names = TRUE)

all_pages <- vector(mode = "list", length = length(scrape))

for (i in seq_along(all_pages)){
  all_pages[[i]] <- read_html(scrape[i]) %>%
    html_elements(css = ".td-excerpt") %>%
    html_text(trim = TRUE)
}

all_pages[[1]

```

*Task 5*

What does it mean to crawl?

Literally, crawling means the action of moving along with hands and knees or with the body stretched out along along a surface, but for online crawling  we intend the process of utilizing a software or automated script to index data on web sites. These automated scripts or programs are sometimes referred to as web crawlers, spiders, spider bots, or just crawlers.

What is it a "web spider"?

A web spider can be also defined as a web crawler - or spiderbot. In essence, a spider is a computer software that gathers data from the World Wide Web. Crawling over websites' pages, it gathers information and indexes it for use later, often for search engine results.

How is it different from a scraper?

The main difference can be found in the function that they fulfill, because meanwhile scraping is about extracting data from one or more websites and only take those data that needs to be extracted, meanwhile crawling helps finding/discovering URLs, storing every piece of information.

Which function(s) should you use to crawl?

We can use the package "rcrawler" - which requires system("java-version").

Let's suppose that we wanto to extracted link from a certain site and we also want to take those links that are external, meaning that they point to other sites, we can use the function:	


```{r, eval=FALSE}
LinkExtractor(url="the_url_of_the_page", ExternalLinks = TRUE)

```

If we wanto to extract parameters and values from our URL we can use the function:


```{r, eval=FALSE}
Linkparameters(URL)

```

However, Rcrawler contains a lot of different functions and features that can be used while crawling and it depends on the type of research that we are trying to conduct.
